	2017-01-26
		- edited normalization of rewards to be linear
		- wrote procedure for adding new external files to be used for code.
		- wrote working draft of QVal recording function, currently examining why the QVals look spiky. Check robocode\misc data\qval plots.xlsx
	2017-01-24
		- writing new QVal recording function
		- writing instructions for adding new files to be imported/exported by the code.
	2017-01-17
	
		- added in ability to clear log at beginning of every run. (should be sufficient memory at the beginning to perform run at every time. It is not necessary to do so but easier this way).
		- commented a bit more in run() and globs.
		- fixed bug where all currSAVs are 0'd for action selection.
	2017-01-14
		- fixed several critical fxnality bugs past weeks (using development branch), including, but not exclusively, in reverse chronological order:
			1. doing double-related calculations for NN purposes with int parameters, resulting in critical loss in resolution.
			2. fixed bug in FP where the actions feeding in were NOT being cycled per feed (ie, same action being fed through all FP possibilities)
			3. Correction in Qfxn; now a Q_prev_target is being generated using Q_prev (unsure what was previously, but correct method should generally be same to the LUT bots.)
		- added fxn comments explaining how FP and BP works, and the tiny tweaks our rendition has.
		- currently putting in a detailed logging fxnality.
		- fixed additional bugs, now in chronological order:
			1. several global arrays and variables not zero'd before being reused (normal usage does not guarentee every field is renewed).
			2. fixed bug where currSAV[4] = myHeading*(4/360) is somehow not being updated (?) by removing the brackets (ok?).
			3. fixed bug where Q_curr is NOT used, causing Q_prev to be always 0.
	2017-01-03
		- fix bug in backprop with SAV[0] treated as only action vector, but other previous fxns treating it as 3.
		- fix bug in numMaxAction
	2016-12-30
		- [done] Import NN framework from NN2.
		- added/corrected some comments
		- fixed a few import export settings for weights.
		- combined all weights into 1 file.
	2016-12-22
		- added readme and worklog files to MyRobots folder.
	2016-12-03 to 2016-12-14
		- implemented NN basic online training structure.
		- implemented NN2 using LUT parameters. 
	2016-12-02
		- working on new states branch.
		- [not used] figure out sine/cosine enemy angle
		- [done] consider implementing reward system based on "change in health difference"
		
	2016-12-01 - a
		hotfixed zeroLUT bug and an oob bug.
		
	2016-11-28 - a
		implementing new state-action pairs. 
		hope to train NN by tonight 
		okay.. don't know how to implement states for the velocity and for the firing action.. not sure what this is
	
	2016-11-26 - j
		- continue testing imp/exp
		- [ignore for now] consider: static flag_error
		- [done] test out zeroLUT
		- tested out import export for LUT, zerolut, WL, all work.
		
		
	2016-11-23 - j
		- test imp/exp settings
		- [done; too much time-wise to string convert both ways] explore possibilities of converting string-reading into hex 
		- `learn about throws, catches, exceptions
		- fixed: multiplefileflag not flipping in between - due to learningloop() invoked prior to export leaving static flag true and carry over
		- [currently nonstatic] consider: changing flag to non-static to allow for proper import if export fails and battle ends.
					- testing as non-staic currently
					?Will this assist in preventing issues from multiple samebot invokes in importing and exporting data?
					we want it to be static so that it will prevent multiple accesses, so that we keep 1 import -> 1 export format, 
					and locked in the use of the import of the file. 
					?Can robot2 export robot1's import? 
					no matter
					the point of flag is also to act as indicator if immediately previous import was successful. therefore should not be static.
		- fixed: sometimes file gets wiped: check if accessing file during beginning of export clears file. - added multiple or's for 
				 stringname, and thus will require editing for every new file added.
	
	2016-11-21 - j
		- [done] switch around error labels to have number first
	
	2016-11-18 - j
		- [done] rewrite imports
		- [done] rewrite exports
		- [done] add config lines in current txt
		- [done] change run fxn

	2016-11-15
		- added rewards for dying and winning 
		- added "onHitBullet" event and "Hit" event to give rewards and to let TF learn. 
		- added two more actions... not sure if any of this makes sense though.. it feels very random. 
		- if gets hit by bullet, it should move away!
	   2:33 pm 
		- execute plan from LUTplan.xlsx
		- update scannedRobot(); 
		- update onHitBullet() event. 
		- update actions
		
	   3:51 pm. 
		- reward system 
			-positiveReward for hitting an enemy
			-negativeReward for getting hit. 
			-negativeTerminalReward
			-positiveTerminalReward
	   1:51 am
		- added states  not really working. 

	2016-11-14
		- currently working on mimicking TrackFire's attacking abilities
		in particular, variables (done), onScannedRobot() (done), generateCurrentStateVector(), doAction()
		
		- implement reward system
		
		- implement simple defensive strategy (eg: Fire.java moves when hit)
		
	  Andy
		- added two more discretized levels for energy and distance: generateCurrentStateVector() (done)
		- added action for doAction()

	

		
	
		

	
	
	december 1, 2016
		
	
	december 2, 2016
		
		
	-to zero data, go to the first line of LUTTrackfire and add 1 to the number. 
	
	This is the one that is used for offline training. 