	2017-07-05
		- A1: (Investigate proper rewarding for NN1)
			on investigating stevenpjg's QLearningNN blob, who had simple uses of change in energy, and hitwall,hitbullet things.
			checked out another NN bot too (kkris(?)), who had complex reward system, couldn't learn anything useful from it.
		- A2/B1: (wavesurfing, targeting)
			surfing: check out basic surfing bot. Targeting: GF, GT (checkout the downloaded jar, alongside repo: http://robocode-archive.strangeautomata.com/robots/)
	2017-06-15
		- Changed strLog refresh to default (instead of only when asked to store log) in both NN1 and 2
		- DONE Continuing import from NN2 to NN1
			- currently fixing error 2 & 10 with winlose and 10 with weights
			- Problem is that weights adjust with similar magnitudes (perhaps initial weights are too large).
				- Maybe there's a problem with the bp algo in that it adjusts the weights too evenly. Solution may be from adjusting weights.
	2017-06-10
		- Brought master up to pace with development on NN2. 
	2017-04-13 to 05-29
		- changing globals to local vars.
		- (04-17 to 04-20) calculating backProp calltime using nanoTime. bp takes around 53ps. interestingly, with log on, takes approx. 26us, 500 times longer.
		- (04-30) bp takes around 10% less time with local vars. Next is to rewrite FP loop since it gets called many times and is where the bottleneck likely is, if there is one close.
		- (05-29) for i = 1000 cycles with local var, first 70*4 rounds are around 150k, and then becomes 6k interestingly.
		- 		  for i = 100 cycles with local var, first 100*4 rounds are around 18k, and then becomes 5k.
		-		  for i = 100 cycles with globs, first 100*4 rounds are around 27k, and then becomes 3k.
		-		  for i = 1000 cycles with globs, first 65*4 rounds are around 180k, and then becomes 6k.
		- 		  I think an effort to use locals should be made.
		
	2017-04-12
		- investigated spiking qvals issue: spikes are small and frequent, likely to do with bullets hitting/getting hit; overall pattern is normal.
	2017-03-28 to 04-10
		- added BPErrors recording functionality.
		- updated new file procedure.
	2017-03-16
		- updated readmes in LUTTrackfire, Dodger, and folder files.
	2017-03-15
		- updated readme in NN2: provides better explanation in intro.
	2017-02-18
		- included introductory comments to NN2.
		- updated readme.txt in main folder.
	2017-02-15
		- edited some comments to NN2.
	2017-01-26
		- edited normalization of rewards to be linear
		- wrote procedure for adding new external files to be used for code.
		- wrote working draft of QVal recording function, currently examining why the QVals look spiky. Check robocode\misc data\qval plots.xlsx
	2017-01-24
		- writing new QVal recording function
		- writing instructions for adding new files to be imported/exported by the code.
	2017-01-17
	
		- added in ability to clear log at beginning of every run. (should be sufficient memory at the beginning to perform run at every time. It is not necessary to do so but easier this way).
		- commented a bit more in run() and globs.
		- fixed bug where all currSAVs are 0'd for action selection.
	2017-01-14
		- fixed several critical fxnality bugs past weeks (using development branch), including, but not exclusively, in reverse chronological order:
			1. doing double-related calculations for NN purposes with int parameters, resulting in critical loss in resolution.
			2. fixed bug in FP where the actions feeding in were NOT being cycled per feed (ie, same action being fed through all FP possibilities)
			3. Correction in Qfxn; now a Q_prev_target is being generated using Q_prev (unsure what was previously, but correct method should generally be same to the LUT bots.)
		- added fxn comments explaining how FP and BP works, and the tiny tweaks our rendition has.
		- currently putting in a detailed logging fxnality.
		- fixed additional bugs, now in chronological order:
			1. several global arrays and variables not zero'd before being reused (normal usage does not guarentee every field is renewed).
			2. fixed bug where currSAV[4] = myHeading*(4/360) is somehow not being updated (?) by removing the brackets (ok?).
			3. fixed bug where Q_curr is NOT used, causing Q_prev to be always 0.
	2017-01-03
		- fix bug in backprop with SAV[0] treated as only action vector, but other previous fxns treating it as 3.
		- fix bug in numMaxAction
	2016-12-30
		- [done] Import NN framework from NN2.
		- added/corrected some comments
		- fixed a few import export settings for weights.
		- combined all weights into 1 file.
	2016-12-22
		- added readme and worklog files to MyRobots folder.
	2016-12-03 to 2016-12-14
		- implemented NN basic online training structure.
		- implemented NN2 using LUT parameters. 
	2016-12-02
		- working on new states branch.
		- [not used] figure out sine/cosine enemy angle
		- [done] consider implementing reward system based on "change in health difference"
		
	2016-12-01 - a
		hotfixed zeroLUT bug and an oob bug.
		
	2016-11-28 - a
		implementing new state-action pairs. 
		hope to train NN by tonight 
		okay.. don't know how to implement states for the velocity and for the firing action.. not sure what this is
	
	2016-11-26 - j
		- continue testing imp/exp
		- [ignore for now] consider: static flag_error
		- [done] test out zeroLUT
		- tested out import export for LUT, zerolut, WL, all work.
		
		
	2016-11-23 - j
		- test imp/exp settings
		- [done; too much time-wise to string convert both ways] explore possibilities of converting string-reading into hex 
		- `learn about throws, catches, exceptions
		- fixed: multiplefileflag not flipping in between - due to learningloop() invoked prior to export leaving static flag true and carry over
		- [currently nonstatic] consider: changing flag to non-static to allow for proper import if export fails and battle ends.
					- testing as non-staic currently
					?Will this assist in preventing issues from multiple samebot invokes in importing and exporting data?
					we want it to be static so that it will prevent multiple accesses, so that we keep 1 import -> 1 export format, 
					and locked in the use of the import of the file. 
					?Can robot2 export robot1's import? 
					no matter
					the point of flag is also to act as indicator if immediately previous import was successful. therefore should not be static.
		- fixed: sometimes file gets wiped: check if accessing file during beginning of export clears file. - added multiple or's for 
				 stringname, and thus will require editing for every new file added.
	
	2016-11-21 - j
		- [done] switch around error labels to have number first
	
	2016-11-18 - j
		- [done] rewrite imports
		- [done] rewrite exports
		- [done] add config lines in current txt
		- [done] change run fxn

	2016-11-15
		- added rewards for dying and winning 
		- added "onHitBullet" event and "Hit" event to give rewards and to let TF learn. 
		- added two more actions... not sure if any of this makes sense though.. it feels very random. 
		- if gets hit by bullet, it should move away!
	   2:33 pm 
		- execute plan from LUTplan.xlsx
		- update scannedRobot(); 
		- update onHitBullet() event. 
		- update actions
		
	   3:51 pm. 
		- reward system 
			-positiveReward for hitting an enemy
			-negativeReward for getting hit. 
			-negativeTerminalReward
			-positiveTerminalReward
	   1:51 am
		- added states  not really working. 

	2016-11-14
		- currently working on mimicking TrackFire's attacking abilities
		in particular, variables (done), onScannedRobot() (done), generateCurrentStateVector(), doAction()
		
		- implement reward system
		
		- implement simple defensive strategy (eg: Fire.java moves when hit)
		
	  Andy
		- added two more discretized levels for energy and distance: generateCurrentStateVector() (done)
		- added action for doAction()

	

		
	
		

	
	
	december 1, 2016
		
	
	december 2, 2016
		
		
	-to zero data, go to the first line of LUTTrackfire and add 1 to the number. 
	
	This is the one that is used for offline training. 
